# Full Training Configuration
# For production training on complete dataset

# Dataset configuration
dataset:
  zarr_path: "${oc.env:CALVIN_DATASET_PATH}/task_ABCD_D/training"
  horizon: 16            # Will be overridden by policy.pred_horizon
  pad_before: 1          # obs_horizon - 1 = 2 - 1
  pad_after: 7           # action_horizon - 1 = 8 - 1
  val_ratio: 0.1
  max_train_episodes: null  # null = use all episodes

# Training hyperparameters
training:
  seed: 42
  batch_size: 128
  num_epochs: 500
  learning_rate: 1.0e-4
  weight_decay: 1.0e-6
  gradient_accumulation_steps: 1
  gradient_clip: 1.0

  # EMA (Exponential Moving Average)
  use_ema: true
  ema_power: 0.75

  # Validation and checkpointing
  val_every: 10
  num_val_steps: 50  # Number of batches for validation
  checkpoint_every: 50
  keep_topk: 3  # Keep top-3 checkpoints by val_loss

  # Logging
  output_dir: "${oc.env:HOME}/forecaster_experiments/${now:%Y-%m-%d}/${now:%H-%M-%S}"
  use_wandb: false
  wandb_project: "langsteer-forecaster"

# Device configuration
device: "cuda"
num_workers: 4
